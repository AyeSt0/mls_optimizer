# -*- coding: utf-8 -*-
"""
MLS Optimizer â€” Patched (colmap v2)
Column mapping (0-based):
  col0: RU (optional)
  col1: EN
  col2: Speaker
  col3: EN-mixed (EN with inline zh/markers)  <-- seed column used by step 20
  col4: ZH_NEW (target)                       <-- all later steps operate here
This file is generated by ChatGPT to fix column layout and pipeline order.
"""

import argparse, json, re, sys, os
import pandas as pd

COL_RU = 0
COL_EN = 1
COL_SPK = 2
COL_SEED = 3  # EN-mixed with identifiers (source of truth before LLM)
COL_ZH  = 4  # target

# Simple placeholder protector with zero-width wrappers
ZW_L = "\u2062"  # Invisible times
ZW_R = "\u2063"  # Invisible separator

# Patterns for protected tokens (order matters: longest first)
PROTECT_PATTERNS = [
    r"\{\{[^}]+\}\}",     # {{var}}
    r"\{[^}]+\}",         # {var}
    r"\[[^\]]+\]",        # [var]
    r"<[^>]+>",           # <tag>
]

def protect_tokens(s: str) -> str:
    if not isinstance(s, str): return s
    def _wrap(m): return f"{ZW_L}{m.group(0)}{ZW_R}"
    for pat in PROTECT_PATTERNS:
        s = re.sub(pat, _wrap, s)
    return s

def load_glossary(glossary_path: str):
    if not glossary_path or not os.path.exists(glossary_path):
        return {}
    with open(glossary_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    # support flat dict or list of dicts with keys "src"/"dst"
    mapping = {}
    if isinstance(data, dict):
        for k, v in data.items():
            if isinstance(v, str):
                mapping[str(k)] = v
    elif isinstance(data, list):
        for item in data:
            if isinstance(item, dict) and "src" in item and "dst" in item:
                mapping[str(item["src"])] = str(item["dst"])
    # longest first to avoid partial overshadow
    items = sorted(mapping.items(), key=lambda kv: len(kv[0]), reverse=True)
    return items

def apply_glossary_seed(text: str, glossary_items):
    if not isinstance(text, str) or not text: return text
    # naive longest-first replace, skip inside protected tokens by using split+join over ZWs later
    # Here we assume tokens were not protected yet; glossary is usually English proper nouns => safe
    for src, dst in glossary_items:
        text = text.replace(src, dst)
    return text

def main():
    ap = argparse.ArgumentParser(description="20_enforce_terms: seed col4 from col3 and apply glossary + token protection")
    ap.add_argument("--excel", required=True)
    ap.add_argument("--sheet", default=None)
    ap.add_argument("--glossary", required=False, help="name_map.json (optional at this step)")
    ap.add_argument("--overwrite", action="store_true", help="overwrite col4 even if already exists")
    ap.add_argument("--out", default=None, help="output path; default overwrite original file")
    args = ap.parse_args()

    df = pd.read_excel(args.excel, sheet_name=args.sheet)
    if df.shape[1] < 5:
        # ensure 5 columns
        extra = 5 - df.shape[1]
        for _ in range(extra):
            df[df.shape[1]] = ""

    glossary_items = load_glossary(args.glossary) if args.glossary else []

    def seed_row(v):
        v = v if isinstance(v, str) else ""
        v = apply_glossary_seed(v, glossary_items) if glossary_items else v
        v = protect_tokens(v)  # protect placeholders so LLM won't touch
        return v

    if args.overwrite:
        df.iloc[:, COL_ZH] = df.iloc[:, COL_SEED].map(seed_row)
    else:
        # fill only empty targets
        mask = df.iloc[:, COL_ZH].isna() | (df.iloc[:, COL_ZH].astype(str).str.len() == 0)
        df.loc[mask, df.columns[COL_ZH]] = df.loc[mask, df.columns[COL_SEED]].map(seed_row)

    out = args.out or args.excel
    df.to_excel(out, index=False)
    print(f"[OK] 20_enforce_terms done. Output -> {out}")

if __name__ == "__main__":
    main()
